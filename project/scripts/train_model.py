"""
BTC ä»·æ ¼å˜åŠ¨é¢„æµ‹ - æ¨¡å‹è®­ç»ƒ

åŸºäºæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾è®­ç»ƒé¢„æµ‹æ¨¡å‹

åŠŸèƒ½:
1. åŠ è½½ç‰¹å¾æ•°æ®
2. æ—¶é—´åºåˆ—äº¤å‰éªŒè¯ï¼ˆä¸éšæœºåˆ†å‰²ï¼‰
3. è®­ç»ƒå¤šç§æ¨¡å‹ (Random Forest, Gradient Boosting, LSTM, etc.)
4. è¯„ä¼°æ¨¡å‹è¡¨ç° (æ–¹å‘å‡†ç¡®ç‡ã€æ¨¡æ‹Ÿæ”¶ç›Š)
5. ä¿å­˜æœ€ä½³æ¨¡å‹

ä½¿ç”¨æ–¹æ³•:
    python train_model.py --data ../data/BTCUSDT_features_1h_*.csv --output ../models
"""

import pandas as pd
import numpy as np
import argparse
import os
import glob
import pickle
import json
import warnings
from datetime import datetime
from typing import Dict, List, Tuple, Optional

# å±è”½ TensorFlow è­¦å‘Š
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
warnings.filterwarnings('ignore')

from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier
from sklearn.linear_model import Ridge, LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import (
    mean_absolute_error, mean_squared_error, r2_score,
    accuracy_score, classification_report, confusion_matrix
)

# TensorFlow/Keras for LSTM
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback
    from tensorflow.keras.optimizers import Adam
    LSTM_AVAILABLE = True
    
    # è‡ªå®šä¹‰æ—¥å¿—å›è°ƒ
    class TrainingLogger(Callback):
        """è®­ç»ƒè¿‡ç¨‹æ—¥å¿—è®°å½•å™¨"""
        def __init__(self, fold_num=None, total_epochs=50):
            super().__init__()
            self.fold_num = fold_num
            self.total_epochs = total_epochs
            self.start_time = None
            
        def on_train_begin(self, logs=None):
            import time
            self.start_time = time.time()
            fold_str = f"Fold {self.fold_num}" if self.fold_num else "æœ€ç»ˆæ¨¡å‹"
            print(f"    [{fold_str}] å¼€å§‹è®­ç»ƒ...")
            
        def on_epoch_end(self, epoch, logs=None):
            import time
            elapsed = time.time() - self.start_time
            loss = logs.get('loss', 0)
            val_loss = logs.get('val_loss', 0)
            
            # æ¯ 5 ä¸ª epoch æˆ–æœ€åä¸€ä¸ª epoch æ‰“å°
            if (epoch + 1) % 5 == 0 or epoch == 0 or epoch == self.total_epochs - 1:
                fold_str = f"Fold {self.fold_num}" if self.fold_num else "Final"
                print(f"    [{fold_str}] Epoch {epoch+1}/{self.total_epochs} - "
                      f"loss: {loss:.4f} - val_loss: {val_loss:.4f} - "
                      f"è€—æ—¶: {elapsed:.1f}s")
        
        def on_train_end(self, logs=None):
            import time
            total_time = time.time() - self.start_time
            fold_str = f"Fold {self.fold_num}" if self.fold_num else "æœ€ç»ˆæ¨¡å‹"
            print(f"    [{fold_str}] è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {total_time:.1f}s")

except ImportError:
    LSTM_AVAILABLE = False
    print("âš ï¸ TensorFlow æœªå®‰è£…ï¼ŒLSTM æ¨¡å‹ä¸å¯ç”¨ã€‚å®‰è£…: pip install tensorflow")


class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    @staticmethod
    def evaluate_regression(y_true: np.ndarray, y_pred: np.ndarray) -> dict:
        """
        è¯„ä¼°å›å½’æ¨¡å‹
        
        Returns:
            åŒ…å«å„é¡¹æŒ‡æ ‡çš„å­—å…¸
        """
        # åŸºæœ¬å›å½’æŒ‡æ ‡
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        r2 = r2_score(y_true, y_pred)
        
        # æ–¹å‘å‡†ç¡®ç‡ï¼ˆæ›´é‡è¦ï¼ï¼‰
        direction_accuracy = np.mean(np.sign(y_true) == np.sign(y_pred))
        
        # æ¨¡æ‹Ÿäº¤æ˜“æ”¶ç›Š
        # å‡è®¾ï¼šé¢„æµ‹æ¶¨å°±åšå¤šï¼Œé¢„æµ‹è·Œå°±åšç©º
        returns = np.where(y_pred > 0, y_true, -y_true)
        total_return = np.sum(returns)
        
        # å¤æ™®æ¯”ç‡
        if np.std(returns) > 0:
            sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252)
        else:
            sharpe = 0
        
        # ç›ˆäºæ¯”
        winning_trades = returns[returns > 0]
        losing_trades = returns[returns < 0]
        if len(losing_trades) > 0 and np.mean(np.abs(losing_trades)) > 0:
            profit_factor = np.sum(winning_trades) / np.abs(np.sum(losing_trades))
        else:
            profit_factor = np.inf if len(winning_trades) > 0 else 0
        
        # èƒœç‡
        win_rate = len(winning_trades) / len(returns) if len(returns) > 0 else 0
        
        return {
            'mae': mae,
            'rmse': rmse,
            'r2': r2,
            'direction_accuracy': direction_accuracy,
            'total_return': total_return,
            'sharpe_ratio': sharpe,
            'profit_factor': profit_factor,
            'win_rate': win_rate,
            'n_trades': len(returns)
        }
    
    @staticmethod
    def evaluate_classification(y_true: np.ndarray, y_pred: np.ndarray, 
                                y_prob: Optional[np.ndarray] = None) -> dict:
        """
        è¯„ä¼°åˆ†ç±»æ¨¡å‹
        """
        accuracy = accuracy_score(y_true, y_pred)
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred)
        
        return {
            'accuracy': accuracy,
            'confusion_matrix': cm.tolist(),
        }


class LSTMPredictor:
    """LSTM ä»·æ ¼é¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, sequence_length: int = 20, task: str = 'regression'):
        """
        Args:
            sequence_length: è¾“å…¥åºåˆ—é•¿åº¦
            task: 'regression' æˆ– 'classification'
        """
        self.sequence_length = sequence_length
        self.task = task
        self.model = None
        self.scaler = StandardScaler()
        self.feature_names = None
        
    def prepare_lstm_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ä¸º LSTM å‡†å¤‡ä¸“é—¨çš„æ—¶åºç‰¹å¾
        
        Args:
            df: åŸå§‹ç‰¹å¾ DataFrame
            
        Returns:
            å¢å¼ºåçš„ç‰¹å¾ DataFrame
        """
        print("  ğŸ”§ LSTM ç‰¹å¾å·¥ç¨‹...")
        
        enhanced_df = df.copy()
        n_original = len(df.columns)
        
        # è·å–ä»·æ ¼ç›¸å…³åˆ— (ç”¨äºè®¡ç®—æ”¶ç›Šç‡)
        price_cols = [col for col in df.columns if 'close' in col.lower() or 'price' in col.lower()]
        
        # 1. æ»åç‰¹å¾ (Lagged Features) - å‰ N ä¸ªæ—¶é—´æ­¥çš„å€¼
        lag_cols = ['1h_kdj_k', '1h_kdj_d', '1h_macd', '1h_rsi_14', '1h_volatility']
        lag_cols = [c for c in lag_cols if c in df.columns]
        
        for col in lag_cols:
            for lag in [1, 3, 5]:
                enhanced_df[f'{col}_lag{lag}'] = df[col].shift(lag).fillna(method='bfill')
        
        # 2. å·®åˆ†ç‰¹å¾ (Difference Features) - å˜åŒ–ç‡
        diff_cols = ['1h_kdj_k', '1h_macd', '1h_rsi_14', '1h_volatility', '1h_vol_ratio_ma20']
        diff_cols = [c for c in diff_cols if c in df.columns]
        
        for col in diff_cols:
            # ä¸€é˜¶å·®åˆ†
            enhanced_df[f'{col}_diff1'] = df[col].diff(1).fillna(0)
            # 5 æ­¥å·®åˆ†
            enhanced_df[f'{col}_diff5'] = df[col].diff(5).fillna(0)
        
        # 3. æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾ (Rolling Statistics)
        roll_cols = ['1h_kdj_k', '1h_macd_hist', '1h_rsi_14']
        roll_cols = [c for c in roll_cols if c in df.columns]
        
        for col in roll_cols:
            # æ»šåŠ¨å‡å€¼
            enhanced_df[f'{col}_roll_mean5'] = df[col].rolling(5).mean().fillna(method='bfill')
            enhanced_df[f'{col}_roll_mean10'] = df[col].rolling(10).mean().fillna(method='bfill')
            # æ»šåŠ¨æ ‡å‡†å·®
            enhanced_df[f'{col}_roll_std5'] = df[col].rolling(5).std().fillna(0)
            # æ»šåŠ¨æœ€å¤§/æœ€å°
            enhanced_df[f'{col}_roll_max5'] = df[col].rolling(5).max().fillna(method='bfill')
            enhanced_df[f'{col}_roll_min5'] = df[col].rolling(5).min().fillna(method='bfill')
        
        # 4. åŠ¨é‡ç‰¹å¾ (Momentum) - å½“å‰å€¼ä¸æ»šåŠ¨å‡å€¼çš„åç¦»
        for col in roll_cols:
            roll_mean = df[col].rolling(10).mean().fillna(method='bfill')
            enhanced_df[f'{col}_momentum'] = (df[col] - roll_mean) / (roll_mean.abs() + 1e-8)
        
        # 5. äº¤å‰ç‰¹å¾ (Cross Features) - æŒ‡æ ‡é—´çš„å…³ç³»
        if '1h_kdj_k' in df.columns and '1h_kdj_d' in df.columns:
            enhanced_df['kdj_spread'] = df['1h_kdj_k'] - df['1h_kdj_d']
            enhanced_df['kdj_spread_change'] = enhanced_df['kdj_spread'].diff(1).fillna(0)
        
        if '1h_macd' in df.columns and '1h_macd_signal' in df.columns:
            enhanced_df['macd_spread'] = df['1h_macd'] - df['1h_macd_signal']
            enhanced_df['macd_spread_change'] = enhanced_df['macd_spread'].diff(1).fillna(0)
        
        # 6. RSI åŒºé—´ç‰¹å¾
        if '1h_rsi_14' in df.columns:
            rsi = df['1h_rsi_14']
            enhanced_df['rsi_zone'] = pd.cut(rsi, bins=[0, 30, 50, 70, 100], labels=[0, 1, 2, 3]).astype(float).fillna(1)
            enhanced_df['rsi_distance_from_50'] = (rsi - 50).abs()
        
        # 7. æ³¢åŠ¨ç‡å˜åŒ–ç‰¹å¾
        if '1h_volatility' in df.columns:
            vol = df['1h_volatility']
            vol_mean = vol.rolling(20).mean().fillna(method='bfill')
            enhanced_df['vol_regime'] = (vol > vol_mean).astype(int)  # é«˜æ³¢åŠ¨/ä½æ³¢åŠ¨
            enhanced_df['vol_change_rate'] = vol.pct_change(5).fillna(0).clip(-10, 10)
        
        # å¤„ç†æ— ç©·å€¼å’Œ NaN
        enhanced_df = enhanced_df.replace([np.inf, -np.inf], 0)
        enhanced_df = enhanced_df.fillna(0)
        
        n_new = len(enhanced_df.columns) - n_original
        print(f"    åŸå§‹ç‰¹å¾: {n_original}, æ–°å¢ç‰¹å¾: {n_new}, æ€»è®¡: {len(enhanced_df.columns)}")
        
        return enhanced_df
    
    def prepare_sequences(self, X: np.ndarray, y: np.ndarray = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        å°†æ•°æ®è½¬æ¢ä¸º LSTM éœ€è¦çš„åºåˆ—æ ¼å¼
        
        Args:
            X: ç‰¹å¾æ•°ç»„ (n_samples, n_features)
            y: ç›®æ ‡æ•°ç»„ (n_samples,)
            
        Returns:
            X_seq: (n_samples - sequence_length, sequence_length, n_features)
            y_seq: (n_samples - sequence_length,)
        """
        X_seq, y_seq = [], []
        
        for i in range(len(X) - self.sequence_length):
            X_seq.append(X[i:i + self.sequence_length])
            if y is not None:
                y_seq.append(y[i + self.sequence_length])
        
        X_seq = np.array(X_seq)
        y_seq = np.array(y_seq) if y is not None else None
        
        return X_seq, y_seq
    
    def build_model(self, input_shape: tuple, n_classes: int = None) -> Sequential:
        """
        æ„å»º LSTM æ¨¡å‹
        
        Args:
            input_shape: (sequence_length, n_features)
            n_classes: åˆ†ç±»ä»»åŠ¡çš„ç±»åˆ«æ•°
        """
        from tensorflow.keras.regularizers import l2
        
        model = Sequential([
            # ç¬¬ä¸€å±‚ LSTM (å¢åŠ æ­£åˆ™åŒ–)
            LSTM(128, return_sequences=True, input_shape=input_shape,
                 kernel_regularizer=l2(0.001), recurrent_regularizer=l2(0.001)),
            BatchNormalization(),
            Dropout(0.3),
            
            # ç¬¬äºŒå±‚ LSTM
            LSTM(64, return_sequences=True,
                 kernel_regularizer=l2(0.001), recurrent_regularizer=l2(0.001)),
            BatchNormalization(),
            Dropout(0.3),
            
            # ç¬¬ä¸‰å±‚ LSTM
            LSTM(32, return_sequences=False,
                 kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Dropout(0.3),
            
            # å…¨è¿æ¥å±‚
            Dense(64, activation='relu', kernel_regularizer=l2(0.001)),
            Dropout(0.2),
            Dense(32, activation='relu', kernel_regularizer=l2(0.001)),
            Dropout(0.1),
        ])
        
        if self.task == 'regression':
            model.add(Dense(1))
            model.compile(
                optimizer=Adam(learning_rate=0.0005),  # é™ä½å­¦ä¹ ç‡
                loss='huber',  # ä½¿ç”¨ Huber lossï¼Œå¯¹å¼‚å¸¸å€¼æ›´é²æ£’
                metrics=['mae']
            )
        else:
            model.add(Dense(n_classes, activation='softmax'))
            model.compile(
                optimizer=Adam(learning_rate=0.0005),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy']
            )
        
        return model
    
    def train(self, X: pd.DataFrame, y: pd.Series, 
              n_splits: int = 5, epochs: int = 50, batch_size: int = 32,
              use_feature_engineering: bool = True) -> dict:
        """
        è®­ç»ƒ LSTM æ¨¡å‹
        
        Args:
            X: ç‰¹å¾ DataFrame
            y: ç›®æ ‡ Series
            n_splits: äº¤å‰éªŒè¯æŠ˜æ•°
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            use_feature_engineering: æ˜¯å¦ä½¿ç”¨ LSTM ä¸“ç”¨ç‰¹å¾å·¥ç¨‹
        """
        if not LSTM_AVAILABLE:
            print("âŒ TensorFlow æœªå®‰è£…ï¼Œè·³è¿‡ LSTM è®­ç»ƒ")
            return {}
        
        # LSTM ä¸“ç”¨ç‰¹å¾å·¥ç¨‹
        if use_feature_engineering:
            X = self.prepare_lstm_features(X)
        
        self.feature_names = X.columns.tolist()
        
        # æ ‡å‡†åŒ–
        X_scaled = self.scaler.fit_transform(X)
        y_values = y.values
        
        # å‡†å¤‡åºåˆ—æ•°æ®
        X_seq, y_seq = self.prepare_sequences(X_scaled, y_values)
        
        if len(X_seq) < 100:
            print("âŒ æ•°æ®é‡ä¸è¶³ä»¥è®­ç»ƒ LSTMï¼ˆéœ€è¦è‡³å°‘ 100 ä¸ªåºåˆ—ï¼‰")
            return {}
        
        print(f"\n{'='*60}")
        print(f"è®­ç»ƒæ¨¡å‹: LSTM (sequence_length={self.sequence_length})")
        print(f"{'='*60}")
        print(f"  åºåˆ—æ•°æ®å½¢çŠ¶: X={X_seq.shape}, y={y_seq.shape}")
        
        # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
        tscv = TimeSeriesSplit(n_splits=n_splits)
        
        fold_metrics = []
        
        # ç¡®å®šç±»åˆ«æ•°ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰
        n_classes = len(np.unique(y_seq)) if self.task == 'classification' else None
        
        print(f"\n  å¼€å§‹ {n_splits} æŠ˜äº¤å‰éªŒè¯...")
        print(f"  æ¯æŠ˜è®­ç»ƒæ ·æœ¬: ~{len(X_seq) // (n_splits + 1) * n_splits}")
        print(f"  æ¯æŠ˜æµ‹è¯•æ ·æœ¬: ~{len(X_seq) // (n_splits + 1)}")
        
        for fold, (train_idx, test_idx) in enumerate(tscv.split(X_seq)):
            X_train, X_test = X_seq[train_idx], X_seq[test_idx]
            y_train, y_test = y_seq[train_idx], y_seq[test_idx]
            
            print(f"\n  === Fold {fold+1}/{n_splits} ===")
            print(f"  è®­ç»ƒé›†: {len(X_train)}, æµ‹è¯•é›†: {len(X_test)}")
            
            # æ„å»ºæ–°æ¨¡å‹ï¼ˆæ¯æŠ˜é‡æ–°æ„å»ºï¼‰
            model = self.build_model(
                input_shape=(self.sequence_length, X.shape[1]),
                n_classes=n_classes
            )
            
            # å›è°ƒå‡½æ•°ï¼ˆåŒ…å«æ—¥å¿—è®°å½•å™¨ï¼‰
            fold_callbacks = [
                TrainingLogger(fold_num=fold+1, total_epochs=epochs),
                EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),
                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=0.00001, verbose=1)
            ]
            
            # è®­ç»ƒ
            model.fit(
                X_train, y_train,
                validation_split=0.1,
                epochs=epochs,
                batch_size=batch_size,
                callbacks=fold_callbacks,
                verbose=0
            )
            
            # é¢„æµ‹
            y_pred = model.predict(X_test, verbose=0)
            
            if self.task == 'regression':
                y_pred = y_pred.flatten()
                metrics = ModelEvaluator.evaluate_regression(y_test, y_pred)
                print(f"  Fold {fold+1}: æ–¹å‘å‡†ç¡®ç‡={metrics['direction_accuracy']:.2%}, "
                      f"MAE={metrics['mae']:.4f}, æ”¶ç›Š={metrics['total_return']:.2f}%")
            else:
                y_pred_class = np.argmax(y_pred, axis=1)
                metrics = ModelEvaluator.evaluate_classification(y_test, y_pred_class)
                print(f"  Fold {fold+1}: å‡†ç¡®ç‡={metrics['accuracy']:.2%}")
            
            fold_metrics.append(metrics)
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = {}
        for key in fold_metrics[0].keys():
            if key != 'confusion_matrix':
                values = [m[key] for m in fold_metrics]
                avg_metrics[key] = np.mean(values)
                avg_metrics[f'{key}_std'] = np.std(values)
        
        # ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        print(f"\n  {'='*50}")
        print(f"  ä½¿ç”¨å…¨éƒ¨ {len(X_seq)} ä¸ªæ ·æœ¬è®­ç»ƒæœ€ç»ˆ LSTM æ¨¡å‹...")
        print(f"  {'='*50}")
        
        self.model = self.build_model(
            input_shape=(self.sequence_length, X.shape[1]),
            n_classes=n_classes
        )
        
        # æœ€ç»ˆæ¨¡å‹çš„å›è°ƒ
        final_callbacks = [
            TrainingLogger(fold_num=None, total_epochs=epochs),
            EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),
            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=0.00001, verbose=1)
        ]
        
        self.model.fit(
            X_seq, y_seq,
            validation_split=0.1,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=final_callbacks,
            verbose=0
        )
        
        return avg_metrics
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """é¢„æµ‹"""
        X_scaled = self.scaler.transform(X)
        X_seq, _ = self.prepare_sequences(X_scaled)
        
        predictions = self.model.predict(X_seq, verbose=0)
        
        if self.task == 'regression':
            return predictions.flatten()
        else:
            return np.argmax(predictions, axis=1)
    
    def save(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        # ä¿å­˜ Keras æ¨¡å‹
        model_path = filepath.replace('.pkl', '_lstm.keras')
        self.model.save(model_path)
        
        # ä¿å­˜å…¶ä»–å‚æ•°
        params = {
            'scaler': self.scaler,
            'sequence_length': self.sequence_length,
            'task': self.task,
            'feature_names': self.feature_names,
            'model_path': model_path
        }
        with open(filepath, 'wb') as f:
            pickle.dump(params, f)
        
        print(f"ğŸ’¾ LSTM æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    @classmethod
    def load(cls, filepath: str) -> 'LSTMPredictor':
        """åŠ è½½æ¨¡å‹"""
        with open(filepath, 'rb') as f:
            params = pickle.load(f)
        
        predictor = cls(
            sequence_length=params['sequence_length'],
            task=params['task']
        )
        predictor.scaler = params['scaler']
        predictor.feature_names = params['feature_names']
        predictor.model = tf.keras.models.load_model(params['model_path'])
        
        return predictor


class EnsemblePredictor:
    """
    æ¨¡å‹èåˆé¢„æµ‹å™¨: GBM åˆ¤æ–­æ–¹å‘ + LSTM é¢„æµ‹å¹…åº¦
    
    ç­–ç•¥:
    1. GBM é¢„æµ‹æ–¹å‘ï¼ˆå‡†ç¡®ç‡æ›´é«˜ï¼Œæ›´ç¨³å®šï¼‰
    2. LSTM é¢„æµ‹å¹…åº¦ï¼ˆæ•æ‰æ—¶åºæ¨¡å¼ï¼‰
    3. åªæœ‰å½“ä¸¤è€…ä¸€è‡´ä¸”ç½®ä¿¡åº¦é«˜æ—¶æ‰äº¤æ˜“
    """
    
    def __init__(self, gbm_weight: float = 0.6, lstm_weight: float = 0.4,
                 confidence_threshold: float = 0.3):
        """
        Args:
            gbm_weight: GBM é¢„æµ‹æƒé‡
            lstm_weight: LSTM é¢„æµ‹æƒé‡
            confidence_threshold: äº¤æ˜“ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆé¢„æµ‹æ¶¨è·Œå¹… > æ­¤å€¼æ‰äº¤æ˜“ï¼‰
        """
        self.gbm_weight = gbm_weight
        self.lstm_weight = lstm_weight
        self.confidence_threshold = confidence_threshold
        
        self.gbm_model = None
        self.lstm_model = None
        self.scaler = StandardScaler()
        self.feature_names = None
        
    def train(self, X: pd.DataFrame, y: pd.Series, 
              n_splits: int = 5, lstm_epochs: int = 50,
              lstm_seq_len: int = 20) -> dict:
        """
        è®­ç»ƒèåˆæ¨¡å‹
        """
        print("\n" + "="*60)
        print("ğŸ”€ è®­ç»ƒèåˆæ¨¡å‹ (GBM + LSTM)")
        print("="*60)
        print(f"   GBM æƒé‡: {self.gbm_weight}, LSTM æƒé‡: {self.lstm_weight}")
        print(f"   ç½®ä¿¡åº¦é˜ˆå€¼: {self.confidence_threshold}")
        
        self.feature_names = X.columns.tolist()
        
        # 1. è®­ç»ƒ GBM æ¨¡å‹
        print("\nğŸ“ˆ ç¬¬ä¸€æ­¥: è®­ç»ƒ GBM æ¨¡å‹...")
        self.gbm_model = PriceMovementPredictor(
            selected_models=['gbm'],
            model_params={'gbm_estimators': 150, 'gbm_lr': 0.08, 'gbm_depth': 6}
        )
        gbm_results = self.gbm_model.train(X, y, n_splits=n_splits)
        
        # 2. è®­ç»ƒ LSTM æ¨¡å‹
        if LSTM_AVAILABLE:
            print("\nğŸ§  ç¬¬äºŒæ­¥: è®­ç»ƒ LSTM æ¨¡å‹...")
            self.lstm_model = LSTMPredictor(
                sequence_length=lstm_seq_len,
                task='regression'
            )
            lstm_results = self.lstm_model.train(
                X, y, n_splits=n_splits, 
                epochs=lstm_epochs, batch_size=128,
                use_feature_engineering=True
            )
        else:
            print("\nâš ï¸ TensorFlow æœªå®‰è£…ï¼Œåªä½¿ç”¨ GBM")
            lstm_results = {}
        
        # 3. è¯„ä¼°èåˆæ•ˆæœï¼ˆæ— æ•°æ®æ³„éœ²ï¼‰
        print("\nğŸ“Š ç¬¬ä¸‰æ­¥: è¯„ä¼°èåˆæ¨¡å‹ï¼ˆæ— æ•°æ®æ³„éœ²ï¼‰...")
        ensemble_results = self._evaluate_ensemble(
            X, y, n_splits, 
            lstm_epochs=lstm_epochs, 
            lstm_seq_len=lstm_seq_len
        )
        
        return {
            'gbm': gbm_results,
            'lstm': lstm_results,
            'ensemble': ensemble_results
        }
    
    def _evaluate_ensemble(self, X: pd.DataFrame, y: pd.Series, 
                          n_splits: int = 5, lstm_epochs: int = 30,
                          lstm_seq_len: int = 20) -> dict:
        """
        è¯„ä¼°èåˆæ¨¡å‹æ•ˆæœï¼ˆæ— æ•°æ®æ³„éœ²ç‰ˆæœ¬ï¼‰
        
        æ¯ä¸ª fold å•ç‹¬è®­ç»ƒ GBM å’Œ LSTMï¼Œç¡®ä¿æµ‹è¯•æ•°æ®ä»æœªè¢«æ¨¡å‹è§è¿‡
        """
        
        tscv = TimeSeriesSplit(n_splits=n_splits)
        
        fold_metrics = []
        
        print(f"\n  å¼€å§‹ {n_splits} æŠ˜äº¤å‰éªŒè¯ï¼ˆæ— æ•°æ®æ³„éœ²ï¼‰...")
        
        for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
            print(f"\n  === Ensemble Fold {fold+1}/{n_splits} ===")
            
            X_train, X_test = X.iloc[train_idx].copy(), X.iloc[test_idx].copy()
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
            
            # 1. åœ¨å½“å‰ fold çš„è®­ç»ƒé›†ä¸Šè®­ç»ƒ GBM
            print(f"    è®­ç»ƒ GBM (è®­ç»ƒé›†: {len(X_train)})...")
            fold_gbm = GradientBoostingRegressor(
                n_estimators=150, learning_rate=0.08, max_depth=6, random_state=42
            )
            gbm_scaler = StandardScaler()
            X_train_scaled = gbm_scaler.fit_transform(X_train)
            X_test_scaled = gbm_scaler.transform(X_test)
            fold_gbm.fit(X_train_scaled, y_train.values)
            gbm_pred = fold_gbm.predict(X_test_scaled)
            
            # 2. åœ¨å½“å‰ fold çš„è®­ç»ƒé›†ä¸Šè®­ç»ƒ LSTM
            if LSTM_AVAILABLE:
                print(f"    è®­ç»ƒ LSTM (è®­ç»ƒé›†: {len(X_train)})...")
                
                # LSTM ç‰¹å¾å·¥ç¨‹
                fold_lstm_extractor = LSTMPredictor(sequence_length=lstm_seq_len, task='regression')
                X_train_lstm = fold_lstm_extractor.prepare_lstm_features(X_train)
                X_test_lstm = fold_lstm_extractor.prepare_lstm_features(X_test)
                
                # æ ‡å‡†åŒ–
                lstm_scaler = StandardScaler()
                X_train_lstm_scaled = lstm_scaler.fit_transform(X_train_lstm)
                X_test_lstm_scaled = lstm_scaler.transform(X_test_lstm)
                
                # å‡†å¤‡åºåˆ—
                X_train_seq, y_train_seq = fold_lstm_extractor.prepare_sequences(
                    X_train_lstm_scaled, y_train.values
                )
                X_test_seq, _ = fold_lstm_extractor.prepare_sequences(X_test_lstm_scaled)
                
                if len(X_train_seq) > 100 and len(X_test_seq) > 0:
                    # æ„å»ºå¹¶è®­ç»ƒ LSTM
                    from tensorflow.keras.regularizers import l2
                    lstm_model = Sequential([
                        LSTM(64, return_sequences=True, input_shape=(lstm_seq_len, X_train_lstm.shape[1]),
                             kernel_regularizer=l2(0.001)),
                        BatchNormalization(),
                        Dropout(0.3),
                        LSTM(32, return_sequences=False, kernel_regularizer=l2(0.001)),
                        BatchNormalization(),
                        Dropout(0.2),
                        Dense(16, activation='relu'),
                        Dense(1)
                    ])
                    lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='huber', metrics=['mae'])
                    
                    # è®­ç»ƒï¼ˆé™é»˜æ¨¡å¼ï¼‰
                    lstm_model.fit(
                        X_train_seq, y_train_seq,
                        epochs=lstm_epochs, batch_size=64,
                        validation_split=0.1, verbose=0,
                        callbacks=[EarlyStopping(patience=5, restore_best_weights=True)]
                    )
                    
                    lstm_pred_raw = lstm_model.predict(X_test_seq, verbose=0).flatten()
                    
                    # å¯¹é½é¢„æµ‹é•¿åº¦
                    offset = len(gbm_pred) - len(lstm_pred_raw)
                    if offset > 0:
                        gbm_pred_aligned = gbm_pred[offset:]
                        y_test_aligned = y_test.values[offset:]
                        lstm_pred = lstm_pred_raw
                    else:
                        gbm_pred_aligned = gbm_pred
                        y_test_aligned = y_test.values
                        lstm_pred = lstm_pred_raw[:len(gbm_pred)]
                else:
                    gbm_pred_aligned = gbm_pred
                    y_test_aligned = y_test.values
                    lstm_pred = gbm_pred
            else:
                gbm_pred_aligned = gbm_pred
                y_test_aligned = y_test.values
                lstm_pred = gbm_pred
            
            # èåˆé¢„æµ‹
            ensemble_pred = self._fuse_predictions(gbm_pred_aligned, lstm_pred)
            
            # åº”ç”¨ç½®ä¿¡åº¦è¿‡æ»¤
            filtered_pred, filtered_true, n_trades = self._apply_confidence_filter(
                ensemble_pred, y_test_aligned
            )
            
            if len(filtered_pred) > 0:
                metrics = ModelEvaluator.evaluate_regression(filtered_true, filtered_pred)
                metrics['n_trades'] = n_trades
                metrics['trade_ratio'] = n_trades / len(ensemble_pred)
            else:
                metrics = {
                    'direction_accuracy': 0.5,
                    'mae': 0,
                    'total_return': 0,
                    'n_trades': 0,
                    'trade_ratio': 0
                }
            
            fold_metrics.append(metrics)
            
            print(f"  Fold {fold+1}: æ–¹å‘å‡†ç¡®ç‡={metrics['direction_accuracy']:.2%}, "
                  f"æ”¶ç›Š={metrics['total_return']:.2f}%, "
                  f"äº¤æ˜“æ¬¡æ•°={metrics['n_trades']}/{len(ensemble_pred)} ({metrics['trade_ratio']:.1%})")
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = {}
        for key in ['direction_accuracy', 'mae', 'total_return', 'n_trades', 'trade_ratio']:
            if key in fold_metrics[0]:
                values = [m.get(key, 0) for m in fold_metrics]
                avg_metrics[key] = np.mean(values)
                avg_metrics[f'{key}_std'] = np.std(values)
        
        print(f"\n  èåˆæ¨¡å‹å¹³å‡ç»“æœ:")
        print(f"    æ–¹å‘å‡†ç¡®ç‡: {avg_metrics['direction_accuracy']:.2%} (Â±{avg_metrics['direction_accuracy_std']:.2%})")
        print(f"    æ€»æ”¶ç›Š: {avg_metrics['total_return']:.2f}% (Â±{avg_metrics['total_return_std']:.2f}%)")
        print(f"    äº¤æ˜“æ¯”ä¾‹: {avg_metrics['trade_ratio']:.1%}")
        
        return avg_metrics
    
    def _fuse_predictions(self, gbm_pred: np.ndarray, lstm_pred: np.ndarray) -> np.ndarray:
        """
        èåˆ GBM å’Œ LSTM é¢„æµ‹
        
        ç­–ç•¥:
        - åŠ æƒå¹³å‡
        - å¦‚æœæ–¹å‘ä¸ä¸€è‡´ï¼Œé™ä½é¢„æµ‹å¹…åº¦ï¼ˆè¡¨ç¤ºä¸ç¡®å®šï¼‰
        """
        # åŠ æƒå¹³å‡
        fused = self.gbm_weight * gbm_pred + self.lstm_weight * lstm_pred
        
        # æ–¹å‘ä¸€è‡´æ€§æ£€æŸ¥
        gbm_direction = np.sign(gbm_pred)
        lstm_direction = np.sign(lstm_pred)
        direction_agree = gbm_direction == lstm_direction
        
        # æ–¹å‘ä¸ä¸€è‡´æ—¶ï¼Œé™ä½é¢„æµ‹å¹…åº¦ 50%
        fused = np.where(direction_agree, fused, fused * 0.5)
        
        return fused
    
    def _apply_confidence_filter(self, predictions: np.ndarray, 
                                  y_true: np.ndarray) -> Tuple[np.ndarray, np.ndarray, int]:
        """
        åº”ç”¨ç½®ä¿¡åº¦è¿‡æ»¤ï¼Œåªä¿ç•™é«˜ç½®ä¿¡åº¦é¢„æµ‹
        
        Returns:
            filtered_pred, filtered_true, n_trades
        """
        # åªäº¤æ˜“é¢„æµ‹å¹…åº¦è¶…è¿‡é˜ˆå€¼çš„ä¿¡å·
        mask = np.abs(predictions) > self.confidence_threshold
        
        filtered_pred = predictions[mask]
        filtered_true = y_true[mask]
        n_trades = np.sum(mask)
        
        return filtered_pred, filtered_true, n_trades
    
    def predict(self, X: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """
        é¢„æµ‹
        
        Returns:
            (predictions, confidence) - é¢„æµ‹å€¼å’Œç½®ä¿¡åº¦
        """
        gbm_pred = self.gbm_model.predict(X)
        
        if self.lstm_model is not None and len(X) > self.lstm_model.sequence_length:
            # ä¸º LSTM å‡†å¤‡å¢å¼ºç‰¹å¾
            X_lstm = self.lstm_model.prepare_lstm_features(X.copy())
            X_lstm_scaled = self.lstm_model.scaler.transform(X_lstm)
            X_seq, _ = self.lstm_model.prepare_sequences(X_lstm_scaled)
            lstm_pred = self.lstm_model.model.predict(X_seq, verbose=0).flatten()
            
            offset = len(gbm_pred) - len(lstm_pred)
            if offset > 0:
                gbm_pred = gbm_pred[offset:]
        else:
            lstm_pred = gbm_pred
        
        # èåˆé¢„æµ‹
        fused = self._fuse_predictions(gbm_pred, lstm_pred)
        
        # è®¡ç®—ç½®ä¿¡åº¦ (æ–¹å‘ä¸€è‡´æ€§ + é¢„æµ‹å¹…åº¦)
        gbm_dir = np.sign(gbm_pred)
        lstm_dir = np.sign(lstm_pred)
        direction_agree = (gbm_dir == lstm_dir).astype(float)
        magnitude_confidence = np.abs(fused) / (np.abs(fused).max() + 1e-8)
        
        confidence = 0.5 * direction_agree + 0.5 * magnitude_confidence
        
        return fused, confidence
    
    def save(self, filepath: str):
        """ä¿å­˜èåˆæ¨¡å‹"""
        # ä¿å­˜ GBM
        gbm_path = filepath.replace('.pkl', '_gbm.pkl')
        self.gbm_model.save(gbm_path)
        
        # ä¿å­˜ LSTM
        if self.lstm_model is not None:
            lstm_path = filepath.replace('.pkl', '_lstm.pkl')
            self.lstm_model.save(lstm_path)
            lstm_saved = True
        else:
            lstm_path = None
            lstm_saved = False
        
        # ä¿å­˜èåˆå‚æ•°
        params = {
            'gbm_weight': self.gbm_weight,
            'lstm_weight': self.lstm_weight,
            'confidence_threshold': self.confidence_threshold,
            'gbm_path': gbm_path,
            'lstm_path': lstm_path,
            'lstm_saved': lstm_saved,
            'feature_names': self.feature_names
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(params, f)
        
        print(f"ğŸ’¾ èåˆæ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    @classmethod
    def load(cls, filepath: str) -> 'EnsemblePredictor':
        """åŠ è½½èåˆæ¨¡å‹"""
        with open(filepath, 'rb') as f:
            params = pickle.load(f)
        
        predictor = cls(
            gbm_weight=params['gbm_weight'],
            lstm_weight=params['lstm_weight'],
            confidence_threshold=params['confidence_threshold']
        )
        predictor.feature_names = params['feature_names']
        
        # åŠ è½½ GBM
        predictor.gbm_model = PriceMovementPredictor.load(params['gbm_path'])
        
        # åŠ è½½ LSTM
        if params['lstm_saved'] and LSTM_AVAILABLE:
            predictor.lstm_model = LSTMPredictor.load(params['lstm_path'])
        
        return predictor


class PriceMovementPredictor:
    """ä»·æ ¼å˜åŠ¨é¢„æµ‹å™¨ - å›å½’æ¨¡å‹"""
    
    def __init__(self, selected_models: List[str] = None, model_params: dict = None):
        """
        Args:
            selected_models: è¦è®­ç»ƒçš„æ¨¡å‹åˆ—è¡¨ ['rf', 'gbm', 'ridge']
            model_params: æ¨¡å‹å‚æ•°å­—å…¸
        """
        params = model_params or {}
        
        all_models = {
            'rf': RandomForestRegressor(
                n_estimators=params.get('rf_estimators', 100), 
                max_depth=params.get('rf_depth', 10), 
                min_samples_split=10,
                random_state=42,
                n_jobs=-1
            ),
            'gbm': GradientBoostingRegressor(
                n_estimators=params.get('gbm_estimators', 100), 
                learning_rate=params.get('gbm_lr', 0.1),
                max_depth=params.get('gbm_depth', 5),
                random_state=42
            ),
            'ridge': Ridge(alpha=params.get('ridge_alpha', 1.0))
        }
        
        # åªä¿ç•™é€‰ä¸­çš„æ¨¡å‹
        if selected_models:
            self.models = {k: v for k, v in all_models.items() if k in selected_models}
        else:
            self.models = all_models
        
        self.scaler = StandardScaler()
        self.best_model = None
        self.best_model_name = None
        self.feature_names = None
        self.feature_importance = None
    
    def train(self, X: pd.DataFrame, y: pd.Series, 
              n_splits: int = 5) -> Dict[str, dict]:
        """
        ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯è®­ç»ƒæ¨¡å‹
        
        Args:
            X: ç‰¹å¾ DataFrame
            y: ç›®æ ‡ Series
            n_splits: äº¤å‰éªŒè¯æŠ˜æ•°
            
        Returns:
            å„æ¨¡å‹çš„è¯„ä¼°ç»“æœ
        """
        self.feature_names = X.columns.tolist()
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_scaled = self.scaler.fit_transform(X)
        
        # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
        tscv = TimeSeriesSplit(n_splits=n_splits)
        
        results = {}
        
        for name, model in self.models.items():
            print(f"\n{'='*60}")
            print(f"è®­ç»ƒæ¨¡å‹: {name}")
            print('='*60)
            
            fold_metrics = []
            
            for fold, (train_idx, test_idx) in enumerate(tscv.split(X_scaled)):
                X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]
                y_train, y_test = y.iloc[train_idx].values, y.iloc[test_idx].values
                
                # è®­ç»ƒ
                model.fit(X_train, y_train)
                
                # é¢„æµ‹
                y_pred = model.predict(X_test)
                
                # è¯„ä¼°
                metrics = ModelEvaluator.evaluate_regression(y_test, y_pred)
                fold_metrics.append(metrics)
                
                print(f"  Fold {fold+1}: æ–¹å‘å‡†ç¡®ç‡={metrics['direction_accuracy']:.2%}, "
                      f"MAE={metrics['mae']:.4f}, æ”¶ç›Š={metrics['total_return']:.2f}%")
            
            # è®¡ç®—å¹³å‡æŒ‡æ ‡
            avg_metrics = {}
            for key in fold_metrics[0].keys():
                if key != 'confusion_matrix':
                    values = [m[key] for m in fold_metrics]
                    avg_metrics[key] = np.mean(values)
                    avg_metrics[f'{key}_std'] = np.std(values)
            
            results[name] = avg_metrics
            
            print(f"\n  å¹³å‡ç»“æœ:")
            print(f"    æ–¹å‘å‡†ç¡®ç‡: {avg_metrics['direction_accuracy']:.2%} "
                  f"(Â±{avg_metrics['direction_accuracy_std']:.2%})")
            print(f"    MAE: {avg_metrics['mae']:.4f} (Â±{avg_metrics['mae_std']:.4f})")
            print(f"    æ€»æ”¶ç›Š: {avg_metrics['total_return']:.2f}% "
                  f"(Â±{avg_metrics['total_return_std']:.2f}%)")
            print(f"    å¤æ™®æ¯”ç‡: {avg_metrics['sharpe_ratio']:.2f}")
            print(f"    èƒœç‡: {avg_metrics['win_rate']:.2%}")
        
        # é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºæ–¹å‘å‡†ç¡®ç‡ï¼‰
        self.best_model_name = max(results, key=lambda x: results[x]['direction_accuracy'])
        self.best_model = self.models[self.best_model_name]
        
        # ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒæœ€ä½³æ¨¡å‹
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {self.best_model_name}")
        print("   ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒ...")
        self.best_model.fit(X_scaled, y.values)
        
        # ç‰¹å¾é‡è¦æ€§
        if hasattr(self.best_model, 'feature_importances_'):
            self.feature_importance = dict(zip(
                self.feature_names, 
                self.best_model.feature_importances_
            ))
        
        return results
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """é¢„æµ‹"""
        X_scaled = self.scaler.transform(X)
        return self.best_model.predict(X_scaled)
    
    def get_top_features(self, n: int = 10) -> List[Tuple[str, float]]:
        """è·å–æœ€é‡è¦çš„ç‰¹å¾"""
        if self.feature_importance is None:
            return []
        
        sorted_features = sorted(
            self.feature_importance.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        return sorted_features[:n]
    
    def save(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        model_data = {
            'best_model': self.best_model,
            'best_model_name': self.best_model_name,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'feature_importance': self.feature_importance
        }
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    @classmethod
    def load(cls, filepath: str) -> 'PriceMovementPredictor':
        """åŠ è½½æ¨¡å‹"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        predictor = cls()
        predictor.best_model = model_data['best_model']
        predictor.best_model_name = model_data['best_model_name']
        predictor.scaler = model_data['scaler']
        predictor.feature_names = model_data['feature_names']
        predictor.feature_importance = model_data['feature_importance']
        
        return predictor


class PriceMovementClassifier:
    """ä»·æ ¼å˜åŠ¨åˆ†ç±»å™¨"""
    
    def __init__(self, selected_models: List[str] = None, model_params: dict = None):
        """
        Args:
            selected_models: è¦è®­ç»ƒçš„æ¨¡å‹åˆ—è¡¨ ['rf', 'gbm', 'ridge']
            model_params: æ¨¡å‹å‚æ•°å­—å…¸
        """
        params = model_params or {}
        
        all_models = {
            'rf': RandomForestClassifier(
                n_estimators=params.get('rf_estimators', 100), 
                max_depth=params.get('rf_depth', 10),
                min_samples_split=10,
                class_weight='balanced',
                random_state=42,
                n_jobs=-1
            ),
            'gbm': GradientBoostingClassifier(
                n_estimators=params.get('gbm_estimators', 100),
                learning_rate=params.get('gbm_lr', 0.1),
                max_depth=params.get('gbm_depth', 5),
                random_state=42
            ),
            'ridge': LogisticRegression(  # åˆ†ç±»ä¸­ç”¨ LogisticRegression ä»£æ›¿ Ridge
                C=1.0 / params.get('ridge_alpha', 1.0),  # C = 1/alpha
                class_weight='balanced',
                max_iter=1000,
                random_state=42
            )
        }
        
        # åªä¿ç•™é€‰ä¸­çš„æ¨¡å‹
        if selected_models:
            self.models = {k: v for k, v in all_models.items() if k in selected_models}
        else:
            self.models = all_models
        
        self.scaler = StandardScaler()
        self.best_model = None
        self.best_model_name = None
        self.feature_names = None
        self.classes = None
    
    def train(self, X: pd.DataFrame, y: pd.Series, 
              n_splits: int = 5) -> Dict[str, dict]:
        """è®­ç»ƒåˆ†ç±»æ¨¡å‹"""
        self.feature_names = X.columns.tolist()
        self.classes = np.unique(y)
        
        X_scaled = self.scaler.fit_transform(X)
        tscv = TimeSeriesSplit(n_splits=n_splits)
        
        results = {}
        
        for name, model in self.models.items():
            print(f"\n{'='*60}")
            print(f"è®­ç»ƒåˆ†ç±»æ¨¡å‹: {name}")
            print('='*60)
            
            fold_accuracies = []
            
            for fold, (train_idx, test_idx) in enumerate(tscv.split(X_scaled)):
                X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]
                y_train, y_test = y.iloc[train_idx].values, y.iloc[test_idx].values
                
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                
                accuracy = accuracy_score(y_test, y_pred)
                fold_accuracies.append(accuracy)
                
                print(f"  Fold {fold+1}: å‡†ç¡®ç‡={accuracy:.2%}")
            
            avg_accuracy = np.mean(fold_accuracies)
            std_accuracy = np.std(fold_accuracies)
            
            results[name] = {
                'accuracy': avg_accuracy,
                'accuracy_std': std_accuracy
            }
            
            print(f"\n  å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.2%} (Â±{std_accuracy:.2%})")
        
        # é€‰æ‹©æœ€ä½³æ¨¡å‹
        self.best_model_name = max(results, key=lambda x: results[x]['accuracy'])
        self.best_model = self.models[self.best_model_name]
        
        print(f"\nğŸ† æœ€ä½³åˆ†ç±»æ¨¡å‹: {self.best_model_name}")
        print("   ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒ...")
        self.best_model.fit(X_scaled, y.values)
        
        return results
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """é¢„æµ‹ç±»åˆ«"""
        X_scaled = self.scaler.transform(X)
        return self.best_model.predict(X_scaled)
    
    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """é¢„æµ‹æ¦‚ç‡"""
        X_scaled = self.scaler.transform(X)
        return self.best_model.predict_proba(X_scaled)
    
    def save(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        model_data = {
            'best_model': self.best_model,
            'best_model_name': self.best_model_name,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'classes': self.classes
        }
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"ğŸ’¾ åˆ†ç±»æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")


def load_data(data_path: str) -> pd.DataFrame:
    """åŠ è½½ç‰¹å¾æ•°æ®"""
    # æ”¯æŒé€šé…ç¬¦
    if '*' in data_path:
        files = glob.glob(data_path)
        if not files:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶: {data_path}")
        # ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
        data_path = max(files, key=os.path.getmtime)
    
    print(f"ğŸ“‚ åŠ è½½æ•°æ®: {data_path}")
    df = pd.read_csv(data_path)
    print(f"   æ ·æœ¬æ•°é‡: {len(df)}")
    print(f"   ç‰¹å¾æ•°é‡: {len(df.columns)}")
    
    return df


def prepare_features(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:
    """
    å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆåŒ…å«å®Œæ•´çš„æ•°æ®æ¸…æ´—ï¼‰
    
    Returns:
        (X, y_regression, y_classification)
    """
    print("\nğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—...")
    
    # æ’é™¤éç‰¹å¾åˆ—
    exclude_cols = [
        'target_regression', 'target_classification', 'target_direction',
        'base_timestamp', 'timestamp', 'close_price'
    ]
    
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    
    X = df[feature_cols].copy()
    y_reg = df['target_regression'] if 'target_regression' in df.columns else None
    y_cls = df['target_classification'] if 'target_classification' in df.columns else None
    
    # 1. ç»Ÿè®¡åŸå§‹æ•°æ®è´¨é‡
    n_samples_original = len(X)
    n_nan_original = X.isna().sum().sum()
    n_inf_original = np.isinf(X.select_dtypes(include=[np.number])).sum().sum()
    
    print(f"   åŸå§‹æ ·æœ¬æ•°: {n_samples_original}")
    print(f"   åŸå§‹ NaN æ•°é‡: {n_nan_original}")
    print(f"   åŸå§‹ Inf æ•°é‡: {n_inf_original}")
    
    # 2. å¤„ç†æ— ç©·å€¼ (æ›¿æ¢ä¸º NaNï¼Œåç»­ç»Ÿä¸€å¤„ç†)
    X = X.replace([np.inf, -np.inf], np.nan)
    
    # 3. åˆ é™¤ NaN æ¯”ä¾‹è¿‡é«˜çš„åˆ— (>50%)
    nan_ratio = X.isna().sum() / len(X)
    high_nan_cols = nan_ratio[nan_ratio > 0.5].index.tolist()
    if high_nan_cols:
        print(f"   âš ï¸ åˆ é™¤é«˜ NaN åˆ— ({len(high_nan_cols)}): {high_nan_cols[:5]}...")
        X = X.drop(columns=high_nan_cols)
    
    # 4. å¤„ç†ç¼ºå¤±å€¼ - ä½¿ç”¨ä¸­ä½æ•°å¡«å……ï¼ˆæ¯”ç”¨ 0 æ›´åˆç†ï¼‰
    for col in X.columns:
        if X[col].isna().any():
            median_val = X[col].median()
            if pd.isna(median_val):
                median_val = 0
            X[col] = X[col].fillna(median_val)
    
    # 5. å¼‚å¸¸å€¼å¤„ç† - ä½¿ç”¨ IQR æ–¹æ³•è£å‰ª
    numeric_cols = X.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        Q1 = X[col].quantile(0.01)
        Q3 = X[col].quantile(0.99)
        X[col] = X[col].clip(Q1, Q3)
    
    # 6. åˆ é™¤æ ‡ç­¾ä¸ºç©ºçš„è¡Œ
    if y_reg is not None:
        valid_idx = ~y_reg.isna()
        X = X[valid_idx].reset_index(drop=True)
        y_reg = y_reg[valid_idx].reset_index(drop=True)
        if y_cls is not None:
            y_cls = y_cls[valid_idx].reset_index(drop=True)
    
    # 7. åˆ é™¤å¸¸é‡åˆ—ï¼ˆæ–¹å·®ä¸º 0 çš„åˆ—æ²¡æœ‰é¢„æµ‹ä»·å€¼ï¼‰
    constant_cols = X.columns[X.std() == 0].tolist()
    if constant_cols:
        print(f"   âš ï¸ åˆ é™¤å¸¸é‡åˆ— ({len(constant_cols)}): {constant_cols[:5]}...")
        X = X.drop(columns=constant_cols)
    
    # 8. æœ€ç»ˆæ•°æ®è´¨é‡æ£€æŸ¥
    n_samples_final = len(X)
    n_nan_final = X.isna().sum().sum()
    
    print(f"\nğŸ“Š æ•°æ®æ¸…æ´—å®Œæˆ:")
    print(f"   æ¸…æ´—åæ ·æœ¬æ•°: {n_samples_final} (åˆ é™¤ {n_samples_original - n_samples_final})")
    print(f"   æ¸…æ´—åç‰¹å¾æ•°: {len(X.columns)}")
    print(f"   å‰©ä½™ NaN æ•°é‡: {n_nan_final}")
    
    return X, y_reg, y_cls


def main():
    parser = argparse.ArgumentParser(description='BTC ä»·æ ¼å˜åŠ¨é¢„æµ‹ - æ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--data', type=str, required=True, help='ç‰¹å¾æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒé€šé…ç¬¦ï¼‰')
    parser.add_argument('--output', type=str, default='../models', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--task', type=str, choices=['regression', 'classification', 'both'], 
                        default='both', help='ä»»åŠ¡ç±»å‹')
    parser.add_argument('--cv-splits', type=int, default=5, help='äº¤å‰éªŒè¯æŠ˜æ•°')
    
    # æ¨¡å‹é€‰æ‹©
    parser.add_argument('--models', type=str, default='all',
                        help='è¦è®­ç»ƒçš„æ¨¡å‹ï¼Œé€—å·åˆ†éš”ã€‚å¯é€‰: rf,gbm,ridge,lstm,ensemble æˆ– all (é»˜è®¤: all)')
    
    # èåˆæ¨¡å‹å‚æ•°
    parser.add_argument('--ensemble-gbm-weight', type=float, default=0.6, help='èåˆæ¨¡å‹ä¸­ GBM æƒé‡')
    parser.add_argument('--ensemble-lstm-weight', type=float, default=0.4, help='èåˆæ¨¡å‹ä¸­ LSTM æƒé‡')
    parser.add_argument('--ensemble-threshold', type=float, default=0.3, help='èåˆæ¨¡å‹ç½®ä¿¡åº¦é˜ˆå€¼')
    
    # LSTM å‚æ•°
    parser.add_argument('--lstm-seq-len', type=int, default=20, help='LSTM åºåˆ—é•¿åº¦')
    parser.add_argument('--lstm-epochs', type=int, default=50, help='LSTM è®­ç»ƒè½®æ•°')
    parser.add_argument('--lstm-batch-size', type=int, default=32, help='LSTM æ‰¹æ¬¡å¤§å°')
    
    # ä¼ ç»Ÿæ¨¡å‹å‚æ•°
    parser.add_argument('--rf-estimators', type=int, default=100, help='Random Forest æ ‘æ•°é‡')
    parser.add_argument('--rf-depth', type=int, default=10, help='Random Forest æœ€å¤§æ·±åº¦')
    parser.add_argument('--gbm-estimators', type=int, default=100, help='GBM æ ‘æ•°é‡')
    parser.add_argument('--gbm-lr', type=float, default=0.1, help='GBM å­¦ä¹ ç‡')
    parser.add_argument('--gbm-depth', type=int, default=5, help='GBM æœ€å¤§æ·±åº¦')
    parser.add_argument('--ridge-alpha', type=float, default=1.0, help='Ridge æ­£åˆ™åŒ–å‚æ•°')
    
    args = parser.parse_args()
    
    # è§£ææ¨¡å‹åˆ—è¡¨
    if args.models.lower() == 'all':
        selected_models = ['rf', 'gbm', 'ridge', 'lstm']
    else:
        selected_models = [m.strip().lower() for m in args.models.split(',')]
    
    # éªŒè¯æ¨¡å‹åç§°
    valid_models = ['rf', 'gbm', 'ridge', 'lstm', 'ensemble']
    for m in selected_models:
        if m not in valid_models:
            print(f"âŒ æ— æ•ˆçš„æ¨¡å‹åç§°: {m}")
            print(f"   å¯é€‰æ¨¡å‹: {', '.join(valid_models)}")
            return
    
    print(f"ğŸ¯ é€‰æ‹©çš„æ¨¡å‹: {', '.join(selected_models)}")
    
    # å‡†å¤‡æ¨¡å‹å‚æ•°
    model_params = {
        'rf_estimators': args.rf_estimators,
        'rf_depth': args.rf_depth,
        'gbm_estimators': args.gbm_estimators,
        'gbm_lr': args.gbm_lr,
        'gbm_depth': args.gbm_depth,
        'ridge_alpha': args.ridge_alpha,
    }
    
    # åˆ†ç¦»ä¼ ç»Ÿæ¨¡å‹ã€LSTM å’Œèåˆæ¨¡å‹
    traditional_models = [m for m in selected_models if m not in ['lstm', 'ensemble']]
    train_lstm = 'lstm' in selected_models
    train_ensemble = 'ensemble' in selected_models
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    df = load_data(args.data)
    
    # å‡†å¤‡ç‰¹å¾
    X, y_reg, y_cls = prepare_features(df)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # å›å½’ä»»åŠ¡ - ä¼ ç»Ÿæ¨¡å‹
    if args.task in ['regression', 'both'] and y_reg is not None and traditional_models:
        print("\n" + "="*60)
        print("ğŸ“ˆ å›å½’ä»»åŠ¡: é¢„æµ‹ä»·æ ¼å˜åŠ¨ç™¾åˆ†æ¯”")
        print("="*60)
        
        predictor = PriceMovementPredictor(
            selected_models=traditional_models,
            model_params=model_params
        )
        results = predictor.train(X, y_reg, n_splits=args.cv_splits)
        
        if predictor.best_model is not None:
            # æ‰“å°ç‰¹å¾é‡è¦æ€§
            print("\nğŸ” Top 10 é‡è¦ç‰¹å¾:")
            for i, (feat, imp) in enumerate(predictor.get_top_features(10), 1):
                print(f"   {i}. {feat}: {imp:.4f}")
            
            # ä¿å­˜æ¨¡å‹
            model_path = os.path.join(args.output, f'regression_model_{timestamp}.pkl')
            predictor.save(model_path)
            
            # ä¿å­˜ç»“æœ
            results_path = os.path.join(args.output, f'regression_results_{timestamp}.json')
            with open(results_path, 'w') as f:
                json.dump(results, f, indent=2)
            print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
    
    # åˆ†ç±»ä»»åŠ¡ - ä¼ ç»Ÿæ¨¡å‹
    if args.task in ['classification', 'both'] and y_cls is not None and traditional_models:
        print("\n" + "="*60)
        print("ğŸ“Š åˆ†ç±»ä»»åŠ¡: é¢„æµ‹ä»·æ ¼å˜åŠ¨åŒºé—´")
        print("="*60)
        
        # è¿‡æ»¤æ‰ NaN ç±»åˆ«
        valid_cls_idx = ~y_cls.isna()
        X_cls = X[valid_cls_idx]
        y_cls_valid = y_cls[valid_cls_idx].astype(int)
        
        print(f"\næ ‡ç­¾åˆ†å¸ƒ:")
        labels = {0: 'å¤§è·Œ(<-2%)', 1: 'å°è·Œ(-2%~-0.5%)', 2: 'æ¨ªç›˜(-0.5%~0.5%)', 
                  3: 'å°æ¶¨(0.5%~2%)', 4: 'å¤§æ¶¨(>2%)'}
        for label, name in labels.items():
            count = (y_cls_valid == label).sum()
            pct = count / len(y_cls_valid) * 100
            print(f"   {name}: {count} ({pct:.1f}%)")
        
        classifier = PriceMovementClassifier(
            selected_models=traditional_models,
            model_params=model_params
        )
        results = classifier.train(X_cls, y_cls_valid, n_splits=args.cv_splits)
        
        if classifier.best_model is not None:
            # ä¿å­˜æ¨¡å‹
            model_path = os.path.join(args.output, f'classification_model_{timestamp}.pkl')
            classifier.save(model_path)
    
    # LSTM æ¨¡å‹è®­ç»ƒ
    if train_lstm and LSTM_AVAILABLE:
        print("\n" + "="*60)
        print("ğŸ§  LSTM æ¨¡å‹è®­ç»ƒ")
        print("="*60)
        
        # LSTM å›å½’
        if args.task in ['regression', 'both'] and y_reg is not None:
            print("\nğŸ“ˆ LSTM å›å½’ä»»åŠ¡:")
            
            lstm_reg = LSTMPredictor(
                sequence_length=args.lstm_seq_len, 
                task='regression'
            )
            lstm_results = lstm_reg.train(
                X, y_reg, 
                n_splits=args.cv_splits,
                epochs=args.lstm_epochs
            )
            
            if lstm_results:
                print(f"\n  å¹³å‡ç»“æœ:")
                print(f"    æ–¹å‘å‡†ç¡®ç‡: {lstm_results.get('direction_accuracy', 0):.2%} "
                      f"(Â±{lstm_results.get('direction_accuracy_std', 0):.2%})")
                print(f"    MAE: {lstm_results.get('mae', 0):.4f} "
                      f"(Â±{lstm_results.get('mae_std', 0):.4f})")
                print(f"    æ€»æ”¶ç›Š: {lstm_results.get('total_return', 0):.2f}% "
                      f"(Â±{lstm_results.get('total_return_std', 0):.2f}%)")
                
                # ä¿å­˜ LSTM æ¨¡å‹
                lstm_path = os.path.join(args.output, f'lstm_regression_{timestamp}.pkl')
                lstm_reg.save(lstm_path)
                
                # ä¿å­˜ç»“æœ
                lstm_results_path = os.path.join(args.output, f'lstm_regression_results_{timestamp}.json')
                with open(lstm_results_path, 'w') as f:
                    json.dump(lstm_results, f, indent=2)
        
        # LSTM åˆ†ç±»
        if args.task in ['classification', 'both'] and y_cls is not None:
            print("\nğŸ“Š LSTM åˆ†ç±»ä»»åŠ¡:")
            
            valid_cls_idx = ~y_cls.isna()
            X_cls = X[valid_cls_idx]
            y_cls_valid = y_cls[valid_cls_idx].astype(int)
            
            lstm_cls = LSTMPredictor(
                sequence_length=args.lstm_seq_len, 
                task='classification'
            )
            lstm_cls_results = lstm_cls.train(
                X_cls, y_cls_valid, 
                n_splits=args.cv_splits,
                epochs=args.lstm_epochs
            )
            
            if lstm_cls_results:
                print(f"\n  å¹³å‡å‡†ç¡®ç‡: {lstm_cls_results.get('accuracy', 0):.2%} "
                      f"(Â±{lstm_cls_results.get('accuracy_std', 0):.2%})")
                
                # ä¿å­˜ LSTM åˆ†ç±»æ¨¡å‹
                lstm_cls_path = os.path.join(args.output, f'lstm_classification_{timestamp}.pkl')
                lstm_cls.save(lstm_cls_path)
    
    elif train_lstm and not LSTM_AVAILABLE:
        print("\nâš ï¸ è·³è¿‡ LSTM è®­ç»ƒï¼šTensorFlow æœªå®‰è£…")
        print("   å®‰è£…å‘½ä»¤: pip install tensorflow")
    
    # èåˆæ¨¡å‹è®­ç»ƒ
    if train_ensemble and args.task in ['regression', 'both'] and y_reg is not None:
        print("\n" + "="*60)
        print("ğŸ”€ èåˆæ¨¡å‹è®­ç»ƒ (GBM + LSTM)")
        print("="*60)
        
        ensemble = EnsemblePredictor(
            gbm_weight=args.ensemble_gbm_weight,
            lstm_weight=args.ensemble_lstm_weight,
            confidence_threshold=args.ensemble_threshold
        )
        
        ensemble_results = ensemble.train(
            X, y_reg,
            n_splits=args.cv_splits,
            lstm_epochs=args.lstm_epochs,
            lstm_seq_len=args.lstm_seq_len
        )
        
        # ä¿å­˜èåˆæ¨¡å‹
        ensemble_path = os.path.join(args.output, f'ensemble_model_{timestamp}.pkl')
        ensemble.save(ensemble_path)
        
        # ä¿å­˜ç»“æœ
        ensemble_results_path = os.path.join(args.output, f'ensemble_results_{timestamp}.json')
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        serializable_results = {}
        for key, value in ensemble_results.items():
            if isinstance(value, dict):
                serializable_results[key] = {k: float(v) if isinstance(v, (np.floating, np.integer)) else v 
                                             for k, v in value.items()}
            else:
                serializable_results[key] = value
        with open(ensemble_results_path, 'w') as f:
            json.dump(serializable_results, f, indent=2)
        print(f"ğŸ“Š èåˆæ¨¡å‹ç»“æœå·²ä¿å­˜åˆ°: {ensemble_results_path}")
    
    print("\n" + "="*60)
    print("âœ… è®­ç»ƒå®Œæˆ!")
    print("="*60)


if __name__ == "__main__":
    main()

